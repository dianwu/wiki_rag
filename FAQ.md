# RAG 系統常見問題 (FAQ)

本文檔記錄了有關此 RAG (Retrieval-Augmented Generation) 專案的一些常見問題與核心概念。

---

### Q1: 如何優化 RAG 系統的表現，特別是當系統檢索到文件但 LLM 認為無效時？

這通常是「檢索（Retrieval）」的精準度不足，或是「生成（Generation）」階段對上下文的理解不夠好。可以從以下幾個方面進行優化：

#### 1. 優化檢索（Retrieval）品質
- **調整檢索數量 `k`**：在 `main.py` 中，可以嘗試增加 `search_kwargs={"k": 4}`，讓系統取回更多相關文件，以提供更豐富的上下文。但需注意過多無關資訊可能會干擾 LLM。
- **優化文件分塊（Chunking）策略**：在 `ingest.py` 中，實驗不同的 `chunk_size` 和 `chunk_overlap`。分塊太小會缺乏上下文，太大則會引入雜訊。修改後需要重新導入資料。
- **更換 Embedding 模型**：`all-mpnet-base-v2` 是通用的好模型，但可以參考 [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) 或使用如 Google Embedding API 等更強大的商業模型，以獲得更好的語意理解能力。

#### 2. 增加重新排序（Re-ranking）環節
- 在檢索和生成之間增加一個步驟。先用較大的 `k` 值（如 `k=10`）召回一個候選集，再用一個更精準的 Cross-Encoder 模型對其重排序，只將最相關的 Top 3 文件傳遞給 LLM。

#### 3. 優化生成（Generation）的提示（Prompt）
- 在 `main.py` 中，可以設計更精細的 Prompt，明確指導 LLM 如何一步步思考、如何處理上下文、以及在找不到答案時該如何回應。

---

### Q2: Embedding 模型在 RAG 專案中扮演什麼角色？

Embedding 模型是決定「檢索品質」的基石。可以把它想像成一個超級聰明的圖書館管理員。

它主要在兩個階段工作：

1.  **建立索引 (`ingest.py`)**:
    - 管理員閱讀所有文件片段（chunks），理解其**語意**，並將其轉換為一組數字，稱為「向量（Vector）」。
    - 這些向量被存儲在向量資料庫（ChromaDB）中，形成一個「語意地圖」，意義相近的文件在這個地圖上的位置也相近。

2.  **查詢資料 (`main.py`)**:
    - 當使用者提出問題，管理員同樣理解問題的**語意**，並將其轉換為一個「問題向量」。
    - 接著，拿著這個「問題向量」去「語意地圖」中，尋找距離最近、最相關的幾個文件向量。這就是 `retriever.invoke()` 的核心工作。

**總結**：一個好的 Embedding 模型能更準確地理解文字的細微差別和真實意圖，從而找到真正能回答問題的文件。如果檢索回來的資料不準確（Garbage In），後續的 LLM 也無法生成好的答案（Garbage Out）。

---

### Q3: 在「建立索引」和「查詢資料」時，可以使用不同的 Embedding 模型嗎？

**絕對不能。** 這兩個階段必須使用完全相同的 Embedding 模型。

**原因**：

每個 Embedding 模型都是一個獨立的「**語意座標系統**」。

- **`model-A`** 就像「台北市的地址系統」。
- **`model-B`** 就像「紐約市的地址系統」。

如果你用 `model-A` 建立索引，就等於是把所有文件都標上了「台北市的地址」。然後，如果你用 `model-B` 來查詢，就等於是拿著一個「紐約市的地址」去台北的地址資料庫裡找東西。

這兩個系統完全不相容，查詢結果將會是隨機且無意義的。

因此，為了確保「問題向量」和「文件向量」在同一個座標系統中，`ingest.py` 和 `main.py` 中使用的 `model_name` 必須完全一致。
